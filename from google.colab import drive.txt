from google.colab import drive
drive.mount('/content/drive')
# ... (other imports)

# Enhanced Data Augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# ... (validation and test datagen remain the same)

# ... (train, validation, and test generators setup remains the same)

# Define the model with Batch Normalization
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 3)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    tf.keras.layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    tf.keras.layers.Conv2D(256, kernel_size=(3, 3), activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dropout(rate=0.5),  # Adjust dropout rate as necessary
    tf.keras.layers.BatchNormalization(),

    tf.keras.layers.Dense(7, activation='softmax')
])

# Learning Rate Scheduling
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=0.001,  # Adjust initial learning rate if necessary
    decay_steps=100000,
    decay_rate=0.96,
    staircase=True
)

opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

model.summary()

# Train the model with a learning rate scheduler
history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=40,
    batch_size=32,
    callbacks=[
        EarlyStopping(monitor='val_loss', patience=8, verbose=0),
        tf.keras.callbacks.LearningRateScheduler(lr_schedule)
    ]
)

# ... (plotting of the results remains the same)